{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQCoT77aCrf5"
   },
   "source": [
    "Steps:\n",
    "1. Runtime -> Change Runtime -> Hardware Accelerator -> GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-_YWFZQIBSh3"
   },
   "outputs": [],
   "source": [
    "# To get some stats about the GPU\n",
    "!nvidia-smi\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t2sg0DkUHupC"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10dm0K7_nvk1"
   },
   "outputs": [],
   "source": [
    "folder = '/content/drive/My Drive/ire-major-project/' # use in python code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U0WBugUGwYVE"
   },
   "source": [
    "# Functions for converting text to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "psMFXWq5wpEw"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ha6crAGdwt7K"
   },
   "outputs": [],
   "source": [
    "class ConvertText2Vec():\n",
    "    def __init__(self, max_nb_words, max_sequence_length, embedding_length,\n",
    "            df_data_column_values):\n",
    "        \"\"\"df_data_column_values is typically `df_data['article_text'].values`\"\"\"\n",
    "        self.max_nb_words = max_nb_words\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.embedding_length = embedding_length\n",
    "        self.tokenizer = Tokenizer(\n",
    "            num_words=max_nb_words, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "        self.tokenizer.fit_on_texts(df_data_column_values)\n",
    "        print('Found %s unique tokens.' % len(self.tokenizer.word_index))\n",
    "    \n",
    "    def convert_to_vector(self, df_data_column_values):\n",
    "        \"\"\"df_column_values is of the form `df_data['article_text'].values`\"\"\"\n",
    "        x = self.tokenizer.texts_to_sequences(df_data_column_values)\n",
    "        word_index = self.tokenizer.word_index\n",
    "        x = pad_sequences(x, maxlen=self.max_sequence_length)\n",
    "        print('Shape of data tensor:', x.shape)\n",
    "        return x, word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBt-PZBsoBN1"
   },
   "source": [
    "# Preprocessing\n",
    "Use these filenames to change the dataset for data\n",
    "    # [this is smallest] articles-training-byarticle-20181122.xml - 3mb\n",
    "    # articles-validation-bypublisher-20181122.xml - 894mb\n",
    "    # articles-training-bypublisher-20181122.xml - 3gb\n",
    "Use these filenames to change the dataset for truth\n",
    "    # [this is smallest] ground-truth-training-byarticle-20181122.xml - 109kb\n",
    "    # ground-truth-validation-bypublisher-20181122.xml - 24mb\n",
    "    # ground-truth-training-bypublisher-20181122.xml - 100mb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZP5wfvbYF_wy"
   },
   "source": [
    "Run the following cell **ONLY ONCE** to save all the parsed dataset files in a csv format in your drive (directly loading from raw files takes lot of time plus a lot of extra memory also due to some reason)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nav66iG5l4bd"
   },
   "source": [
    "### 1. Load (Training and Testing) data ~ 36 secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gW_cSL43pHNR"
   },
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(folder+'data_training_bypublisher.csv')\n",
    "df_truth = pd.read_csv(folder+'ground_truth_training_bypublisher.csv')\n",
    "df_data.info(memory_usage='deep')\n",
    "df_truth.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhxEGWazlmPy"
   },
   "source": [
    "Loading Testing data separately (if we are not splitting it out of training data) ~ 10 secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fam7_6FMmAq2"
   },
   "outputs": [],
   "source": [
    "df_val_data = pd.read_csv(folder+'data_validation_bypublisher.csv')\n",
    "df_val_truth = pd.read_csv(folder+'ground_truth_validation_bypublisher.csv')\n",
    "df_val_data.info(memory_usage='deep')\n",
    "df_val_truth.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BFLiI-Cya9q"
   },
   "source": [
    "### 2. Set the Parameters\n",
    "Do some analysis and set them accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBUrg15BwrRJ"
   },
   "outputs": [],
   "source": [
    "# # The maximum number of words to be used. (most frequent)\n",
    "# MAX_NB_WORDS = 50000\n",
    "# # Max number of words in each complaint.\n",
    "# MAX_SEQUENCE_LENGTH = 500\n",
    "# # This is fixed.\n",
    "# EMBEDDING_DIM = 100\n",
    "\n",
    "MAX_NB_WORDS = 50000  # dictionary size\n",
    "MAX_SEQUENCE_LENGTH = 600  # max word length of each individual article\n",
    "EMBEDDING_DIM = 300  # dimensionality of the embedding vector (50, 100, 200, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLuuQQ2JrQoe"
   },
   "source": [
    "### 2. Coverting Documents to vectors\n",
    "- Total time ~ 15 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZuUn9clvsRpl"
   },
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "\n",
    "text_to_vec_converter = ConvertText2Vec(\n",
    "    MAX_NB_WORDS, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM,\n",
    "    df_data['article_text'].values\n",
    ")\n",
    "\n",
    "\n",
    "# Convert training and testing data to vectors of fixed length (Use one of the 2 cases below)\n",
    "\n",
    "# Case 1) Test data is split from training data only\n",
    "#############################################################################################\n",
    "# x = text_to_vec_converter.convert_to_vector(df_data['article_text'].values)\n",
    "# y_tmp = np.array(df_truth['hyperpartisan'].values)\n",
    "# y = np.array([1 if x == 'true' else 0 for x in y_tmp])\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.10, random_state = 42)\n",
    "\n",
    "\n",
    "# Case 2) Test data is separately loaded\n",
    "#############################################################################################\n",
    "# x_train, word_index_train = text_to_vec_converter.convert_to_vector(df_data['article_text'].values)\n",
    "# x_val, word_index_val = text_to_vec_converter.convert_to_vector(df_val_data['article_text'].values)\n",
    "# x_test, word_index_test = text_to_vec_converter.convert_to_vector(df_test_data['article_text'].values)\n",
    "\n",
    "print(\"converting to vectors took\",time()-start_time,\"to complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pTDCNUJ3f2_z"
   },
   "outputs": [],
   "source": [
    "x_train, word_index_train = text_to_vec_converter.convert_to_vector(df_data['article_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LcIlvbJJiuFx"
   },
   "outputs": [],
   "source": [
    "x_val, word_index_val = text_to_vec_converter.convert_to_vector(df_val_data['article_text'].values)\n",
    "x_test, word_index_test = text_to_vec_converter.convert_to_vector(df_test_data['article_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1TGCW9bRLPiZ"
   },
   "outputs": [],
   "source": [
    "y_tmp = np.array(df_truth['hyperpartisan'].values)\n",
    "y_train = np.array([1 if x == True else 0 for x in y_tmp])\n",
    "y_bias_kind = df_train_truth.bias.values\n",
    "\n",
    "y_tmp = np.array(df_val_truth['hyperpartisan'].values)\n",
    "y_val = np.array([1 if x == True else 0 for x in y_tmp])\n",
    "y_val_bias_kind = df_val_truth.bias.values\n",
    "\n",
    "y_tmp = np.array(df_test_truth['hyperpartisan'].values)\n",
    "y_test = np.array([1 if x == 'true' else 0 for x in y_tmp])\n",
    "y_test_bias_kind = df_test_truth.bias.values\n",
    "\n",
    "NUM_CLASSES_BIAS = len(np.unique(y_bias))\n",
    "NUM_CLASSES_BIAS_KIND = len(np.unique(y_bias_kind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KmME-CzAcVgX"
   },
   "outputs": [],
   "source": [
    "print(y_train[y_train == 1].shape)\n",
    "print(y_train[y_train == 0].shape)\n",
    "print(y_val[y_val == 1].shape)\n",
    "print(y_val[y_val == 0].shape)\n",
    "print(y_test[y_test == 1].shape)\n",
    "print(y_test[y_test == 0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FwYTcP8ZG7WM"
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_bias = to_categorical(y_bias, num_classes=NUM_CLASSES_BIAS)\n",
    "y_test_bias = to_categorical(y_test_bias, num_classes=NUM_CLASSES_BIAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5p0ZHjJJNEr"
   },
   "outputs": [],
   "source": [
    "labelEncoder = LabelEncoder()\n",
    "labelEncoder.fit(np.unique(y_bias_kind))\n",
    "# labelEncoder.classes_\n",
    "y_bias_kind=labelEncoder.transform(y_bias_kind)\n",
    "y_test_bias_kind=labelEncoder.transform(y_test_bias_kind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BYRTnmGs2YWS"
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open(folder + 'glove.6B.300d.txt', 'r', encoding='utf8')\n",
    "for line in f:\n",
    "    # each line starts with a word; rest of the line is the vector\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print(f'Found {len(embeddings_index)} word vectors in glove file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VWtiTbxSWcWa"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index_train) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index_train.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print(\"embedding_matrix shape:\", np.shape(embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ZWQJ0JgaQ9X"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "loaded_embeddings = Embedding(len(word_index_train) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvEv4oIkDyrl"
   },
   "source": [
    "# Model Definition\n",
    "Link to original implementation of CuDNNLSTM - https://github.com/ShawnyXiao/TextClassification-Keras/tree/master/model/TextRNN.\n",
    "\n",
    "It is based on the paper \"Recurrent Neural Network for Text Classification with Multi-Task Learning\" (https://arxiv.org/pdf/1605.05101.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Em3LXL9DqeTU"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Embedding, Dense\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "\n",
    "class TextRNN(Model):\n",
    "    def __init__(self,\n",
    "                 maxlen,\n",
    "                 max_features,\n",
    "                 embedding_dims,\n",
    "                 class_num=1,\n",
    "                 last_activation='sigmoid'):\n",
    "        super(TextRNN, self).__init__()\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "        self.embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)\n",
    "        self.rnn = CuDNNLSTM(128)  # LSTM or GRU\n",
    "        self.classifier = Dense(self.class_num, activation=self.last_activation)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if len(inputs.get_shape()) != 2:\n",
    "            raise ValueError('The rank of inputs of TextRNN must be 2, but now is %d' % len(inputs.get_shape()))\n",
    "        if inputs.get_shape()[1] != self.maxlen:\n",
    "            raise ValueError('The maxlen of inputs of TextRNN must be %d, but now is %d' % (self.maxlen, inputs.get_shape()[1]))\n",
    "        embedding = self.embedding(inputs)\n",
    "        x = self.rnn(embedding)\n",
    "        output = self.classifier(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WorCFnRXEz9y"
   },
   "source": [
    "# Building and Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WABnNfiG9fCJ"
   },
   "source": [
    "Build (define) the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4qx3pul9aLl"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Dropout\n",
    "from keras import Sequential, Model, Input\n",
    "\n",
    "max_features = MAX_NB_WORDS\n",
    "maxlen = MAX_SEQUENCE_LENGTH\n",
    "embedding_dims = EMBEDDING_DIM\n",
    "batch_size = 500\n",
    "epochs = 5\n",
    "\n",
    "print('Build model...')\n",
    "# Model (1) Using CuDNN based approach\n",
    "# -----------------------------------------------------------\n",
    "# model = TextRNN(maxlen, max_features, embedding_dims)\n",
    "# model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# Model (2) Using plain LSTM based approach\n",
    "# -----------------------------------------------------------\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "# model.add(SpatialDropout1D(0.2))\n",
    "# model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Model (3)\n",
    "# input_layer = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "# embedding_layer = loaded_embeddings(input_layer)\n",
    "# embedding_layer = Dropout(0.5)(embedding_layer)\n",
    "\n",
    "# hidden_layer = LSTM(64, recurrent_dropout=0.5)(embedding_layer)\n",
    "# hidden_layer = Dropout(0.5)(hidden_layer)\n",
    "\n",
    "# output_layer = Dense(1, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# model = Model(input_layer, output_layer)\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#               optimizer='adamax',\n",
    "#               metrics=['accuracy'])\n",
    "# print(model.summary())\n",
    "\n",
    "# Model (4)\n",
    "input_layer = Input(shape=(MAX_SEQUENCE_LENGTH, ), dtype='int32')\n",
    "\n",
    "embedding_layer = loaded_embeddings(input_layer)\n",
    "embedding_layer = Dropout(0.5)(embedding_layer)\n",
    "\n",
    "hidden_layer = LSTM(64, recurrent_dropout=0.5)(embedding_layer)\n",
    "hidden_layer = Dropout(0.5)(hidden_layer)\n",
    "\n",
    "# Task 1\n",
    "output_bias = Dense(2, activation='softmax')(hidden_layer)\n",
    "\n",
    "# Task 2\n",
    "output_bias_kind = Dense(5, activation='softmax')(hidden_layer)\n",
    "\n",
    "\n",
    "model = Model(input_layer, [output_bias, output_bias_kind])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adamax', \n",
    "              metrics=['acc'])\n",
    "\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylrzMW_q9r-t"
   },
   "source": [
    "Training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WuPizylqqs83"
   },
   "outputs": [],
   "source": [
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=2, mode='max')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_val, y_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4SlIMqlwE9u0"
   },
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9jlWA0KZE9fK"
   },
   "outputs": [],
   "source": [
    "print('Test...')\n",
    "start_time = time()\n",
    "result = model.predict(x_test)\n",
    "print(\"took\",time()-start_time,\"to complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7p-WmVN2L5z5"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Precision:', precision_score(y_test, y_pred))\n",
    "print('Recall:', recall_score(y_test, y_pred))\n",
    "print('F1 Score:', f1_score(y_test, y_pred))\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZbjO4gEKYw0c"
   },
   "outputs": [],
   "source": [
    "# <dataset_name>_<model_type>_<MAX_NB_WORDS/max_features>_<MAX_SEQUENCE_LENGTH/maxlen>_<EMBEDDING_DIM/embedding_dims>\n",
    "# model.save_weights(folder+'model/bypublisher_training_CuDNNLSTM_50000_250_100')\n",
    "# model.save_weights(folder+'model/bypublishervalidation_CuDNNLSTM_50000_600_100',save_format='tf')\n",
    "model.save_weights(folder+'model/CuDNN/5000_350_100/bypublisher_training_CuDNNLSTM_5000_250_100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5h-oa3ZZrz4"
   },
   "source": [
    "# Loading back a saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dFTRJb74fpGQ"
   },
   "outputs": [],
   "source": [
    "# loaded_model.load_weights(folder+'model/bypublishervalidation_CuDNNLSTM_50000_600_100.data-00000-of-00001')\n",
    "\n",
    "# loaded_model = TextRNN(maxlen, max_features, embedding_dims)\n",
    "# loaded_model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "loaded_model = Sequential()\n",
    "loaded_model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "loaded_model.add(SpatialDropout1D(0.2))\n",
    "loaded_model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "loaded_model.add(Dense(1, activation='sigmoid'))\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "loaded_model.load_weights(folder+'model/CuDNN/5000_350_100/bypublisher_training_CuDNNLSTM_5000_250_100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_SPdRTiDZo5H"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Precision:', precision_score(y_test, y_pred))\n",
    "print('Recall:', recall_score(y_test, y_pred))\n",
    "print('F1 Score:', f1_score(y_test, y_pred))\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ORyK9il6o85t"
   },
   "outputs": [],
   "source": [
    "y_pred_bias, y_pred_bias_kind = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hlHEDHciJcs4"
   },
   "outputs": [],
   "source": [
    "print(classification_report(np.argmax(y_test_bias, axis=1),\n",
    "                            np.argmax(y_pred_bias, axis=1),\n",
    "                            target_names=['unbiased','biased']))\n",
    "\n",
    "print(classification_report(np.argmax(y_test_bias_kind, axis=1),\n",
    "                            np.argmax(y_pred_bias_kind, axis=1),\n",
    "                            target_names=labelEncoder.inverse_transform(reverse_to_categorical(y_train_bias_kind))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X8ejN70UJfr2"
   },
   "outputs": [],
   "source": [
    "model.save('lstm_multitask.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0tq3cYKuJhwb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "wkcz8kcxoFLQ",
    "U0WBugUGwYVE",
    "Nav66iG5l4bd"
   ],
   "name": "Final-model-implementation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
