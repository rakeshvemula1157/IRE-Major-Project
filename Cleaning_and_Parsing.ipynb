{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P3xbF8DjQUn1",
    "outputId": "259fc8a2-991a-42ac-e1a3-cba4048e6e61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6r9QiK0CQs7e"
   },
   "outputs": [],
   "source": [
    "folder = '/content/drive/My Drive/ire-major-project/' # use in python code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jt0c1fbgQxlr"
   },
   "source": [
    "# Functions for Parsing and Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZTPkjrjoQt6Q",
    "outputId": "9932327a-f06f-49ac-9cd4-af62ece3275f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as et\n",
    "import lxml\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "def clean_txt(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove stop words\n",
    "    text = text.split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if w not in stops]\n",
    "    text = \" \".join(text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"\", text)\n",
    "    # Remove non-Latin characters\n",
    "    text = re.sub(\n",
    "        u'[^\\\\x00-\\\\x7F\\\\x80-\\\\xFF\\\\u0100-\\\\u017F\\\\u0180-\\\\u024F\\\\u1E00-\\\\u1EFF]', u'', text)\n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r'[\\W_+]', ' ', text)\n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "def parse_data_xml_helper(article):\n",
    "    # get meta data\n",
    "    id = article.get('id')\n",
    "    date = article.get('published-at')\n",
    "    title = clean_txt(article.get('title'))\n",
    "\n",
    "    \"\"\"\n",
    "    # get external link info\n",
    "    external_links, internal_count=[],0\n",
    "    for link in article.find_all('a'):\n",
    "        if str(link.get('type'))=='internal':\n",
    "            internal_count+=1\n",
    "        else:\n",
    "            external_links.append(link.get('href'))\n",
    "    \"\"\"\n",
    "    # get actual text\n",
    "    article_text = article.get_text()\n",
    "    article_text = clean_txt(article_text)\n",
    "    result = {'id':id, 'date':date, 'title':title, 'article_text':article_text}\n",
    "    return result\n",
    "\n",
    "def get_data_from_xml(filename):\n",
    "    preprocessed_dicts=[]\n",
    "    with open(filename) as f:\n",
    "        for event, element in et.iterparse(f):\n",
    "            if(element.tag=='article'):\n",
    "                i=et.tostring(element)\n",
    "                sp = Soup(i, \"xml\")\n",
    "                i=sp.find('article')\n",
    "                preprocessed_dicts.append(parse_data_xml_helper(i))\n",
    "    return pd.DataFrame(preprocessed_dicts)\n",
    "\n",
    "def parse_truth_xml_helper(article):\n",
    "    # get meta data\n",
    "    id = article.get('id')\n",
    "    hyperpartisan = article.get('hyperpartisan')\n",
    "    bias = article.get('bias')\n",
    "    result = {'id':id,'hyperpartisan':hyperpartisan,'bias':bias}\n",
    "    return result\n",
    "\n",
    "def get_ground_truth_from_xml(filename):\n",
    "    print(\"Loading data from xml\")\n",
    "    xml_file = open(filename).read()\n",
    "    soup = Soup(xml_file, 'lxml')\n",
    "    articles=soup.find_all('article')\n",
    "    print(\"Number of articles: \", len(articles))\n",
    "    truth = []\n",
    "    for a in tqdm(articles):\n",
    "        truth.append(parse_truth_xml_helper(a))        \n",
    "    return pd.DataFrame(truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBt-PZBsoBN1"
   },
   "source": [
    "# Preprocessing\n",
    "Use these filenames to change the dataset for data\n",
    "    # [this is smallest] articles-training-byarticle-20181122.xml - 3mb\n",
    "    # articles-validation-bypublisher-20181122.xml - 894mb\n",
    "    # articles-training-bypublisher-20181122.xml - 3gb\n",
    "Use these filenames to change the dataset for truth\n",
    "    # [this is smallest] ground-truth-training-byarticle-20181122.xml - 109kb\n",
    "    # ground-truth-validation-bypublisher-20181122.xml - 24mb\n",
    "    # ground-truth-training-bypublisher-20181122.xml - 100mb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZP5wfvbYF_wy"
   },
   "source": [
    "Run the following cell **ONLY ONCE** to save all the parsed dataset files in a csv format in your drive (directly loading from raw files takes lot of time plus a lot of extra memory also due to some reason)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ClvcafcF_cj",
    "outputId": "c47c6cd9-cd14-4ead-d28a-6e4946a39533"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 645/645 [00:00<00:00, 182262.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from xml\n",
      "Number of articles:  645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_data = get_data_from_xml(folder+'articles-training-bypublisher-20181122.xml')\n",
    "df_data.to_csv(folder+'data_training_bypublisher.csv')\n",
    "\n",
    "df_truth = get_ground_truth_from_xml(folder+'ground-truth-training-bypublisher-20181122.xml')\n",
    "df_truth.to_csv(folder+'ground_truth_training_bypublisher.csv')\n",
    "\n",
    "df_val_data = get_data_from_xml(folder+'articles-validation-bypublisher-20181122.xml')\n",
    "df_val_data.to_csv(folder+'data_validation_bypublisher.csv')\n",
    "\n",
    "df_val_truth = get_ground_truth_from_xml(folder+'ground-truth-validation-bypublisher-20181122.xml')\n",
    "df_val_truth.to_csv(folder+'ground_truth_validation_bypublisher.csv')\n",
    "\n",
    "df_test_data = get_data_from_xml(folder+'articles-training-byarticle-20181122.xml')\n",
    "df_test_data.to_csv(folder+'data_training_byarticle.csv')\n",
    "\n",
    "df_test_truth = get_ground_truth_from_xml(folder+'ground-truth-training-byarticle-20181122.xml')\n",
    "df_test_truth.to_csv(folder+'ground_truth_training_byarticle.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Cleaning and Parsing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
